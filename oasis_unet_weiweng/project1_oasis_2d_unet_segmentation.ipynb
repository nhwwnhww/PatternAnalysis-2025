{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "226bec76",
      "metadata": {
        "id": "226bec76"
      },
      "source": [
        "# Environment Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c5f15fdb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5f15fdb",
        "outputId": "cc206740-2219-45d5-8cc6-ac5ba2bfeca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch: 2.5.1\n",
            "torch.version.cuda: 12.1\n",
            "cuda.is_available: True\n",
            "device_count: 1\n",
            "device 0: NVIDIA GeForce RTX 3060 Ti capability: (8, 6)\n",
            "\n",
            "=== nvidia-smi ===\n",
            " Fri Oct 17 20:57:39 2025       \n",
            "python: c:\\Users\\nhwen\\anaconda3\\envs\\comp3710\\python.exe\n",
            "platform: Windows-10-10.0.26200-SP0\n"
          ]
        }
      ],
      "source": [
        "import torch, sys, platform, subprocess, os\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"torch.version.cuda:\", torch.version.cuda)\n",
        "print(\"cuda.is_available:\", torch.cuda.is_available())\n",
        "print(\"device_count:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"device 0:\", torch.cuda.get_device_name(0),\n",
        "          \"capability:\", torch.cuda.get_device_capability(0))\n",
        "try:\n",
        "    out = subprocess.check_output(\"nvidia-smi\", shell=True).decode().splitlines()[0]\n",
        "    print(\"\\n=== nvidia-smi ===\\n\", out)\n",
        "except Exception as e:\n",
        "    print(\"nvidia-smi not found / driver not working:\", e)\n",
        "print(\"python:\", sys.executable)\n",
        "print(\"platform:\", platform.platform())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "322dbde0",
      "metadata": {
        "id": "322dbde0"
      },
      "source": [
        "# init Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2337165e",
      "metadata": {
        "id": "2337165e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5ba03179",
      "metadata": {
        "id": "5ba03179"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "NUM_CLASSES = 4  # 背景 + 3 种脑组织\n",
        "IMG_SIZE = 128\n",
        "\n",
        "class OASIS2DPNGDataset(Dataset):\n",
        "    \"\"\"OASIS PNG 2D Brain Slice Dataset for segmentation\"\"\"\n",
        "    def __init__(self, image_dir, mask_dir, img_size=IMG_SIZE, augment=False):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.mask_files = sorted(os.listdir(mask_dir))\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
        "\n",
        "        # Load images\n",
        "        img = Image.open(img_path).convert(\"L\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        # Resize\n",
        "        img = TF.resize(img, [self.img_size, self.img_size])\n",
        "        mask = TF.resize(mask, [self.img_size, self.img_size], interpolation=TF.InterpolationMode.NEAREST)\n",
        "\n",
        "        # To tensor\n",
        "        img = TF.to_tensor(img)  # [1, H, W], float32\n",
        "        mask = np.array(mask, dtype=np.uint8)\n",
        "\n",
        "        # Map mask pixel values to [0, NUM_CLASSES-1]\n",
        "        mask = mask // (256 // NUM_CLASSES)\n",
        "        mask = torch.tensor(mask, dtype=torch.long)  # [H, W]\n",
        "\n",
        "        # Data augmentation\n",
        "        if self.augment:\n",
        "            if random.random() < 0.5:\n",
        "                img = TF.hflip(img); mask = TF.hflip(mask)\n",
        "            if random.random() < 0.5:\n",
        "                img = TF.vflip(img); mask = TF.vflip(mask)\n",
        "\n",
        "        return img, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feb3f835",
      "metadata": {
        "id": "feb3f835"
      },
      "source": [
        "# Create Dataset (DataLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "CNXUA4FadON4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNXUA4FadON4",
        "outputId": "f743c6e7-a664-4c00-e488-a2c5e98c2f5f"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# ROOT = \"/content/drive/MyDrive/COMP3710/OASIS\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "olrZ37m9d6jS",
      "metadata": {
        "id": "olrZ37m9d6jS"
      },
      "outputs": [],
      "source": [
        "# base_path = r\"C:\\COMP3710\\OASIS\"\n",
        "# base_path = ROOT\n",
        "base_path = \"D:\\COMP3710\\OASIS\"\n",
        "IMG_SIZE = 128\n",
        "NUM_CLASSES = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "31712511",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31712511",
        "outputId": "dca4ae81-661e-4083-a174-c670000946cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val/Test samples: 9664/1120/544\n"
          ]
        }
      ],
      "source": [
        "train_dataset = OASIS2DPNGDataset(\n",
        "    os.path.join(base_path, \"keras_png_slices_train\"),\n",
        "    os.path.join(base_path, \"keras_png_slices_seg_train\"),\n",
        "    img_size=IMG_SIZE,\n",
        "    augment=True\n",
        ")\n",
        "val_dataset = OASIS2DPNGDataset(\n",
        "    os.path.join(base_path, \"keras_png_slices_validate\"),\n",
        "    os.path.join(base_path, \"keras_png_slices_seg_validate\"),\n",
        "    img_size=IMG_SIZE,\n",
        "    augment=False\n",
        ")\n",
        "test_dataset = OASIS2DPNGDataset(\n",
        "    os.path.join(base_path, \"keras_png_slices_test\"),\n",
        "    os.path.join(base_path, \"keras_png_slices_seg_test\"),\n",
        "    img_size=IMG_SIZE,\n",
        "    augment=False\n",
        ")\n",
        "if __name__ == \"__main__\":\n",
        "    # train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "    # train_loader = DataLoader(\n",
        "    #   train_dataset,\n",
        "    #   batch_size=8,                # 保持合适 batch size\n",
        "    #   shuffle=True,\n",
        "    #   num_workers=4,               # 提高加载速度\n",
        "    #   pin_memory=True,             # 加速 CPU→GPU 传输\n",
        "    # )\n",
        "    val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "    print(f\"Train/Val/Test samples: {len(train_dataset)}/{len(val_dataset)}/{len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "132bc36b",
      "metadata": {
        "id": "132bc36b"
      },
      "source": [
        "# Model (UNet2D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "07dad272",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07dad272",
        "outputId": "7b0ed423-9b50-4bc2-c3b1-10463b1e3308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model params (M): 7.849124\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "class UNet2D(nn.Module):\n",
        "    def __init__(self, in_c=1, n_classes=4, base=32, act_layer=nn.SiLU):\n",
        "        super().__init__()\n",
        "        def double_conv(ic, oc):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(ic, oc, 3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(oc),\n",
        "                act_layer(),\n",
        "                nn.Conv2d(oc, oc, 3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(oc),\n",
        "                act_layer(),\n",
        "            )\n",
        "        self.inc   = double_conv(in_c, base)\n",
        "        self.down1 = nn.Sequential(nn.MaxPool2d(2), double_conv(base, base*2))\n",
        "        self.down2 = nn.Sequential(nn.MaxPool2d(2), double_conv(base*2, base*4))\n",
        "        self.down3 = nn.Sequential(nn.MaxPool2d(2), double_conv(base*4, base*8))\n",
        "        self.bot   = nn.Sequential(nn.MaxPool2d(2), double_conv(base*8, base*16))\n",
        "        self.up1   = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.c1    = double_conv(base*16 + base*8, base*8)\n",
        "        self.up2   = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.c2    = double_conv(base*8 + base*4, base*4)\n",
        "        self.up3   = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.c3    = double_conv(base*4 + base*2, base*2)\n",
        "        self.up4   = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.c4    = double_conv(base*2 + base, base)\n",
        "        self.outc  = nn.Conv2d(base, n_classes, 1)\n",
        "\n",
        "    def _cat(self, up, skip):\n",
        "        diffY = skip.size(2) - up.size(2)\n",
        "        diffX = skip.size(3) - up.size(3)\n",
        "        up = F.pad(up, [diffX//2, diffX - diffX//2, diffY//2, diffY - diffY//2])\n",
        "        return torch.cat([skip, up], dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.bot(x4)\n",
        "        x = self.c1(self._cat(self.up1(x5), x4))\n",
        "        x = self.c2(self._cat(self.up2(x), x3))\n",
        "        x = self.c3(self._cat(self.up3(x), x2))\n",
        "        x = self.c4(self._cat(self.up4(x), x1))\n",
        "        return self.outc(x)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNet2D(in_c=1, n_classes=NUM_CLASSES).to(device)\n",
        "print(\"Model params (M):\", sum(p.numel() for p in model.parameters())/1e6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93b3f853",
      "metadata": {
        "id": "93b3f853"
      },
      "source": [
        "# Loss, Optimizer, Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1be21bd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1be21bd6",
        "outputId": "8f1df3b4-7ba5-4e65-c26f-601beafc1e32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\nhwen\\AppData\\Local\\Temp\\ipykernel_26216\\3384598616.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=torch.cuda.is_available())\n"
          ]
        }
      ],
      "source": [
        "def dice_loss(logits: torch.Tensor, target: torch.Tensor, eps: float = 1e-6):\n",
        "    \"\"\"\n",
        "    logits: (N, C, H, W)\n",
        "    target: \n",
        "      - 二分类: (N, H, W) 或 (N,1,H,W) 的{0,1}\n",
        "      - 多分类: (N, H, W) 的类别索引(0..C-1)\n",
        "    返回: mean dice loss\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 4, \"logits 应为 (N,C,H,W)\"\n",
        "    N, C, H, W = logits.shape\n",
        "\n",
        "    if target.dim() == 4 and target.shape[1] == 1:\n",
        "        target = target.squeeze(1)         # (N,H,W)\n",
        "    target = target.long()\n",
        "\n",
        "    if C == 1:\n",
        "        # 二分类\n",
        "        probs = torch.sigmoid(logits)      # (N,1,H,W)\n",
        "        tgt = target.float().unsqueeze(1)  # (N,1,H,W), 0/1\n",
        "        inter = (probs * tgt).sum(dim=(0,2,3))\n",
        "        union = probs.sum(dim=(0,2,3)) + tgt.sum(dim=(0,2,3))\n",
        "        dice = (2*inter + eps) / (union + eps)\n",
        "        loss = 1 - dice.mean()\n",
        "        return loss\n",
        "    else:\n",
        "        # 多分类\n",
        "        probs = F.softmax(logits, dim=1)   # (N,C,H,W)\n",
        "        tgt_1h = F.one_hot(target, num_classes=C).permute(0,3,1,2).float()  # (N,C,H,W)\n",
        "        dims = (0,2,3)\n",
        "        inter = (probs * tgt_1h).sum(dim=dims)   # (C,)\n",
        "        union = probs.sum(dim=dims) + tgt_1h.sum(dim=dims)  # (C,)\n",
        "        dice = (2*inter + eps) / (union + eps)   # (C,)\n",
        "        loss = 1 - dice.mean()\n",
        "        return loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def dice_score(logits: torch.Tensor, targets: torch.Tensor, exclude_bg: bool = True, eps: float = 1e-6) -> float:\n",
        "    \"\"\"\n",
        "    返回批次平均 Dice（float）\n",
        "    \"\"\"\n",
        "    N, C, H, W = logits.shape\n",
        "    if targets.dim() == 4 and targets.shape[1] == 1:\n",
        "        targets = targets.squeeze(1)\n",
        "    targets = targets.long()\n",
        "\n",
        "    if C == 1:\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs > 0.5).float()\n",
        "        tgt = targets.float().unsqueeze(1)\n",
        "        inter = (preds * tgt).sum(dim=(1,2,3))\n",
        "        denom = preds.sum(dim=(1,2,3)) + tgt.sum(dim=(1,2,3))\n",
        "        dice = ((2*inter + eps) / (denom + eps)).mean()\n",
        "        return dice.item()\n",
        "    else:\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        preds = probs.argmax(dim=1)  # (N,H,W)\n",
        "        preds_oh   = F.one_hot(preds, num_classes=C).permute(0,3,1,2).float()\n",
        "        targets_oh = F.one_hot(targets, num_classes=C).permute(0,3,1,2).float()\n",
        "\n",
        "        inter = (preds_oh * targets_oh).sum(dim=(0,2,3))          # (C,)\n",
        "        denom = preds_oh.sum(dim=(0,2,3)) + targets_oh.sum(dim=(0,2,3))\n",
        "        dice_c = (2*inter + eps) / (denom + eps)                   # (C,)\n",
        "        if exclude_bg and C > 1:\n",
        "            dice_c = dice_c[1:]\n",
        "        return dice_c.mean().item()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "scaler = GradScaler(enabled=torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2a4b4b",
      "metadata": {
        "id": "1f2a4b4b"
      },
      "source": [
        "# Training & Validation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3de6500b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "3de6500b",
        "outputId": "b4ad88bd-b4cc-46af-f669-06e94072778d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 [Train]:   0%|          | 0/1208 [00:00<?, ?it/s]C:\\Users\\nhwen\\AppData\\Local\\Temp\\ipykernel_26216\\150913050.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=torch.cuda.is_available()):\n",
            "                                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 | Train Loss: 0.0560 | Val Loss: 0.0497 | Val Dice: 0.9392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 | Train Loss: 0.0468 | Val Loss: 0.0509 | Val Dice: 0.9351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 | Train Loss: 0.0436 | Val Loss: 0.0479 | Val Dice: 0.9383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/20 | Train Loss: 0.0409 | Val Loss: 0.0522 | Val Dice: 0.9320\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/20 | Train Loss: 0.0392 | Val Loss: 0.0438 | Val Dice: 0.9433\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/20 | Train Loss: 0.0378 | Val Loss: 0.0427 | Val Dice: 0.9446\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/20 | Train Loss: 0.0363 | Val Loss: 0.0400 | Val Dice: 0.9481\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/20 | Train Loss: 0.0357 | Val Loss: 0.0387 | Val Dice: 0.9497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/20 | Train Loss: 0.0350 | Val Loss: 0.0401 | Val Dice: 0.9476\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/20 | Train Loss: 0.0342 | Val Loss: 0.0448 | Val Dice: 0.9413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/20 | Train Loss: 0.0337 | Val Loss: 0.0408 | Val Dice: 0.9466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/20 | Train Loss: 0.0332 | Val Loss: 0.0382 | Val Dice: 0.9501\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/20 | Train Loss: 0.0328 | Val Loss: 0.0378 | Val Dice: 0.9505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/20 | Train Loss: 0.0324 | Val Loss: 0.0375 | Val Dice: 0.9508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/20 | Train Loss: 0.0318 | Val Loss: 0.0363 | Val Dice: 0.9524\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/20 | Train Loss: 0.0317 | Val Loss: 0.0374 | Val Dice: 0.9509\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/20 | Train Loss: 0.0314 | Val Loss: 0.0377 | Val Dice: 0.9505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/20 | Train Loss: 0.0311 | Val Loss: 0.0367 | Val Dice: 0.9519\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/20 | Train Loss: 0.0310 | Val Loss: 0.0368 | Val Dice: 0.9517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/20 | Train Loss: 0.0306 | Val Loss: 0.0375 | Val Dice: 0.9508\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm  # 用于显示进度条\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    # 使用 tqdm 显示 batch 进度条\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [Train]\", leave=False)\n",
        "    for imgs, masks in loop:\n",
        "        imgs, masks = imgs.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(imgs)\n",
        "            loss = dice_loss(logits, masks)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "        # 实时更新 batch loss\n",
        "        loop.set_postfix({\"batch_loss\": loss.item()})\n",
        "\n",
        "    train_loss /= len(train_dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_dice = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in val_loader:\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            logits = model(imgs)\n",
        "            batch_loss = dice_loss(logits, masks).item()\n",
        "            val_loss += batch_loss * imgs.size(0)\n",
        "            val_dice += dice_score(logits, masks) * imgs.size(0)\n",
        "    val_loss /= len(val_dataset)\n",
        "    val_dice /= len(val_dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Val Dice: {val_dice:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2231a588",
      "metadata": {
        "id": "2231a588"
      },
      "source": [
        "# Test & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0d7e8554",
      "metadata": {
        "id": "0d7e8554"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import amp as torch_amp\n",
        "\n",
        "CLASS_NAMES = [\"bg\", \"class1\", \"class2\", \"class3\"]  # 确保长度 >= NUM_CLASSES\n",
        "AMP_DTYPE = torch.float16\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_per_class_dice(model, loader, num_classes: int, \n",
        "                            exclude_bg: bool = True, \n",
        "                            device: torch.device = None,\n",
        "                            amp_enabled: bool = True):\n",
        "    model.eval()\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "    cm = torch.zeros((num_classes, num_classes), dtype=torch.long, device=device)\n",
        "\n",
        "    for imgs, masks in loader:\n",
        "        imgs  = imgs.to(device, non_blocking=True)\n",
        "        if masks.ndim == 4 and masks.shape[1] == 1:\n",
        "            masks = masks.squeeze(1)\n",
        "        masks = masks.to(device, non_blocking=True).long()  # (N,H,W)\n",
        "\n",
        "        with torch_amp.autocast(device_type=\"cuda\", dtype=AMP_DTYPE, enabled=amp_enabled and device.type==\"cuda\"):\n",
        "            logits = model(imgs)\n",
        "            preds  = logits.argmax(dim=1)  # (N,H,W)\n",
        "\n",
        "        k = (masks * num_classes + preds).view(-1)\n",
        "        cm += torch.bincount(k, minlength=num_classes*num_classes).view(num_classes, num_classes)\n",
        "\n",
        "    TP = cm.diag().to(torch.float32)\n",
        "    FP = cm.sum(0).to(torch.float32) - TP\n",
        "    FN = cm.sum(1).to(torch.float32) - TP\n",
        "    dice_c = (2*TP) / (2*TP + FP + FN + 1e-6)  # (C,)\n",
        "\n",
        "    def print_table(dice_vals, names, title):\n",
        "        print(\"\\n\" + title)\n",
        "        print(\"-\"*len(title))\n",
        "        for name, v in zip(names, dice_vals):\n",
        "            print(f\"{name:>8}: Dice = {float(v):.4f}\")\n",
        "        print(f\"{'MEAN':>8}: Dice = {float(torch.tensor(dice_vals).mean()):.4f}\")\n",
        "\n",
        "    # 含背景\n",
        "    names_all = [CLASS_NAMES[i] if i < len(CLASS_NAMES) else f\"class{i}\" for i in range(num_classes)]\n",
        "    print_table(dice_c.tolist(), names_all, \"Per-class Dice (including background)\")\n",
        "\n",
        "    # 排除背景\n",
        "    if num_classes > 1:\n",
        "        fg = dice_c[1:]\n",
        "        names_fg = names_all[1:]\n",
        "        print_table(fg.tolist(), names_fg, \"Per-class Dice (excluding background)\")\n",
        "\n",
        "        passed = bool((fg >= 0.90).all().item())\n",
        "        print(\"\\n==> \" + (\"PASS ✅ (exclude bg)\" if passed else \"FAIL ❌ (exclude bg)\"),\n",
        "              f\"| min Dice = {float(fg.min()):.4f}\")\n",
        "    else:\n",
        "        passed = bool((dice_c >= 0.90).all().item())\n",
        "        print(\"\\n==> \" + (\"PASS ✅\" if passed else \"FAIL ❌\"),\n",
        "              f\"| min Dice = {float(dice_c.min()):.4f}\")\n",
        "\n",
        "    return dice_c.cpu().tolist(), float(dice_c.mean().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fc9c9a5a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Per-class Dice (including background)\n",
            "-------------------------------------\n",
            "      bg: Dice = 0.9988\n",
            "  class1: Dice = 0.9444\n",
            "  class2: Dice = 0.9514\n",
            "  class3: Dice = 0.9727\n",
            "    MEAN: Dice = 0.9668\n",
            "\n",
            "Per-class Dice (excluding background)\n",
            "-------------------------------------\n",
            "  class1: Dice = 0.9444\n",
            "  class2: Dice = 0.9514\n",
            "  class3: Dice = 0.9727\n",
            "    MEAN: Dice = 0.9562\n",
            "\n",
            "==> PASS ✅ (exclude bg) | min Dice = 0.9444\n",
            "\n",
            "[CALL RESULT]\n",
            "per_class (including bg): [0.9988069534301758, 0.9444249868392944, 0.951388418674469, 0.9727374315261841]\n",
            "mean_dice (including bg): 0.9668394327163696\n",
            "PASS(exclude bg): True | min_fg_dice = 0.9444249868392944\n"
          ]
        }
      ],
      "source": [
        "from torch import amp as torch_amp  # 确保已导入\n",
        "import torch\n",
        "\n",
        "# 如果你用了我上一条消息里的函数，请把它所在的 cell 先执行一遍\n",
        "# 然后运行这个 cell 进行调用与打印：\n",
        "\n",
        "try:\n",
        "    per_class, mean_dice = evaluate_per_class_dice(\n",
        "        model,\n",
        "        test_loader,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        exclude_bg=True,        # 判定是否达标按“排除背景”\n",
        "        device=device,\n",
        "        amp_enabled=True\n",
        "    )\n",
        "    print(\"\\n[CALL RESULT]\", flush=True)\n",
        "    print(\"per_class (including bg):\", per_class, flush=True)\n",
        "    print(\"mean_dice (including bg):\", mean_dice, flush=True)\n",
        "\n",
        "    # 额外做一次达标判断（排除背景）\n",
        "    if len(per_class) > 1:\n",
        "        fg = torch.tensor(per_class[1:], dtype=torch.float32)\n",
        "        passed = bool((fg >= 0.90).all().item())\n",
        "        print(\"PASS(exclude bg):\", passed, \"| min_fg_dice =\", float(fg.min()), flush=True)\n",
        "\n",
        "except Exception as e:\n",
        "    import traceback, sys\n",
        "    print(\"ERROR while evaluating:\", e, file=sys.stderr, flush=True)\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "comp3710",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
